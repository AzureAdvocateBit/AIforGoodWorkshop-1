{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision API\n",
    "\n",
    "This script explores using the Cognitive Services Vision API via its REST API. \n",
    "\n",
    "Here, we'll write an R function to extract a random\n",
    "image from Wikimedia Commons, and another function to generate a caption of the image using the Vision API. You can see the end result in this [blog post](http://blog.revolutionanalytics.com/2018/03/computer-vision-api.html).\n",
    "\n",
    "The concepts are mostly explained as we go, but if you\n",
    "want to find more information, take a look here:\n",
    "\n",
    "* Computer Vision Overview: https://cda.ms/kZ\n",
    "* Computer Vision Documentation: https://cda.ms/m0\n",
    "\n",
    "## Using this Notebook\n",
    "\n",
    "The scripts \n",
    "are provided as Jupyter Notebooks within the [Azure Notebooks](https://notebooks.azure.com/) service.\n",
    "You don't need a Microsoft\n",
    "Account to view the scripts, but you will need to set one up and generate keys in Azure\n",
    "to run\n",
    "the examples. All of the examples use free Azure services.\n",
    "\n",
    "If you're new to Jupyter Notebooks, here's a quick intro:\n",
    "\n",
    "1. Click within a cell, and then press `Ctrl+Enter` to run (or render) the current cell.\n",
    "2. You'll see a number to the left of the cell when the computations are complete, like this: `In [1]:`. (The number represents the order of computations.) If there's output, it will print below the cell. You may have to scroll up to see it all.\n",
    "3. Run each cell, in order, from to bottom.\n",
    "4. To download/upload files, return to the [library view](https://notebooks.azure.com/davidsmi/libraries/qcon) and use the functions in the toolbar.\n",
    "\n",
    "For more information about Notebooks, check out the [Jupyter Notebook documentation](http://jupyter.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "If you're new to R, you might want to start with this [Introduction to\n",
    "R](https://notebooks.azure.com/davidsmi/libraries/intro-r) notebook\n",
    "to get a sense of the language.\n",
    "\n",
    "## First, clone these workshop materials\n",
    "\n",
    "1. Visit https://notebooks.azure.com/davidsmi/libraries/odsc\n",
    "\n",
    "    * Sign in with your Microsoft account if needed\n",
    "\n",
    "1. click Clone in the toolbar, to create a copy of the workshop materials\n",
    "\n",
    "## Connecting to Azure services\n",
    "\n",
    "You will need:\n",
    "\n",
    "1. A [Microsoft account](https://account.microsoft.com/account). You can use an existing Outlook 365\n",
    "or Xbox Live account, or create a new one.\n",
    "\n",
    "1. A Microsoft Azure subscription. If you don't already have an Azure subscription, you can visit\n",
    "[https://cda.ms/kT](https://cda.ms/kT) and also get \\$200 in credits to use with paid services. You'll need to provide\n",
    "a credit or debit card, but everything we'll be doing is free to use. If you're a student, you can \n",
    "also register at [https://cda.ms/kY](https://cda.ms/kY) without a credit card for a \\$100 credit.\n",
    "\n",
    "You'll also need a few other things specific to this workshop. Follow the instructions below to \n",
    "set up everything you need.\n",
    "\n",
    "## Log in to the Azure Portal\n",
    "\n",
    "1. Visit https://portal.azure.com \n",
    "2. Sign in with your Microsoft Account. If you don't have a Microsoft account, use the \n",
    "   links above to create one for free.\n",
    "\n",
    "## Create an Azure Resource Group\n",
    "\n",
    "In Azure, a Resource Group is a collection of services you have created. It groups services\n",
    "together, and makes it easy to bulk-delete things later on. We'll create one for this lab.\n",
    "\n",
    "1. Visit https://portal.azure.com (and sign in if needed)\n",
    "2. Click \"Resource Groups\" in the left column\n",
    "3. Click \"+ Add\"\n",
    "    * Resource Group Name: odsc\n",
    "    * Subscription: _there should be just one option_\n",
    "    * Resource Group Location: South Eastern US\n",
    "4. Click \"Create\"\n",
    "   \n",
    "A notification will appear in the top right. Click the button \"Pin to Dashboard\" to pin this resource group to your home page in the Azure portal, as you'll be referring to it frequently.\n",
    "\n",
    "## Create authorization keys for Computer Vision\n",
    "\n",
    "1. Visit https://portal.azure.com (and sign in if needed)\n",
    "2. Click \"+ Create a Resource\" (top-left corner)\n",
    "3. Click \"AI + Cognitive Services\"\n",
    "4. Click \"Computer Vision API\"\n",
    "    * Name: qcon-vision\n",
    "    * Subscription: _there should be just one option_\n",
    "    * Location: East US\n",
    "    * Pricing Tier: F0 (free, 20 calls per minute)\n",
    "    * Resource Group: Use existing \"qcon\" group\n",
    "5. Click \"Create\"\"\n",
    "\n",
    "\n",
    "## Modify the keys.txt file\n",
    "\n",
    "Download the file `keys.txt` and provide the keys listed from the Azure Portal. \n",
    "\n",
    "(To download this file, highlight it in the Library view and then press `d` or click the download icon in the toolbar.)\n",
    "\n",
    "For the first line\n",
    "of the file, `region`, change it to `eastus`. For the remaining keys, visit your `qcon` resource\n",
    "group in the [Azure Portal](https://portal.azure.com) and then:\n",
    "\n",
    "1. Click on the API resource for Computer Vision `qcon-vision`\n",
    "2. In the menu, click on \"keys\"\n",
    "3. Click the \"copy to clipboard\" next to KEY 1. (You can ignore KEY 2).\n",
    "4. Paste the key into the `vision` entry in keys.txt\n",
    "\n",
    "1. Click on the API resource for Custom Vision `qcon-customvision`\n",
    "2. In the menu, click on \"keys\"\n",
    "3. Click the \"copy to clipboard\" next to KEY 1. (You can ignore KEY 2).\n",
    "4. Paste the key into the `custom` entry in keys.txt\n",
    "\n",
    "1. Click on the API resource for Custom Vision `qcon-customvision_Prediction` (this resource\n",
    "   was created automatically for you).\n",
    "2. In the menu, click on \"keys\"\n",
    "3. Click the \"copy to clipboard\" next to KEY 1. (You can ignore KEY 2).\n",
    "4. Paste the key into the `cvpred` entry in keys.txt\n",
    "\n",
    "\n",
    "Your final `keys.txt` file will look like this, but with different (working) keys:\n",
    "\n",
    "```\n",
    "       key\n",
    "region eastus\n",
    "vision 7f1f01ac24064abd80970f41a90237e7\n",
    "custom 1632b49e2930430694a9bbd3ab0c0cc2\n",
    "cvpred 37eb1f0e5fd34253939350197ae3d933\n",
    "```\n",
    "\n",
    "Once you've done this for all the cognitive services, save the file `keys.txt` and upload it to \n",
    "replace the existing file in Azure Notebooks. To upload the modified file, go to the Library and press the \"+\" (New File) icon or press `n`, click \"From Computer\" > \"Choose Files\" and select the `keys.txt` file on your hard drive, and click \"Upload\". A box saying \"File Exists, Overwrite?\" will appear; click it to continue.\n",
    "\n",
    "## Get started!\n",
    "\n",
    "The R scripts are provided as Jupyter Notebook files (with the `.ipynb` extension). You can tackle\n",
    "the files in any order, but we recommend the following sequence:\n",
    "\n",
    "1. `Vision API.ipynb`: Explore analyzing images from Wikimedia Commons using the Microsoft Vision API\n",
    "2. `Custom Vision API.ipynb`: Create the \"Not Hotdog\" image recognition application featured in _Silicon Valley_\n",
    "\n",
    "### More examples\n",
    "\n",
    "For more examples of using the Cognitive Services APIs from R, take a look a the\n",
    "following blog posts. R code is included in the posts or in linked Github repositories.\n",
    "\n",
    "* [Emotions in Video](https://blog.exploratory.io/analyzing-emotions-using-facial-expressions-in-video-with-microsoft-ai-and-r-8f7585dd0780). _The Economist_ compared\n",
    "the emotions of Donald Trump and Hillary Clinton from video of the 2016 Presidential Debates.\n",
    "* [Gender distribution of programmers](http://blog.revolutionanalytics.com/2016/06/programmers-gender.html). Using the Face API to infer apparent gender of programmers from Github avatars.\n",
    "* [Grading lumber by identifying wood knots](http://blog.revolutionanalytics.com/2017/09/wood-knots.html). Training a neural network with transfer learning.\n",
    "* [Sentiment analysis of Amazon reviews](http://blog.revolutionanalytics.com/2017/08/text-featurization-microsoftml.html). Uses the text featurization functions of Microsoft ML Server. YOu can't run these from this Notebook, but you can use the [Data Science Virtual Machine](https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/?WT.mc_id=Qcon-notebook-davidsmi) instead. \n",
    "* [Identifying snow leopards](http://blog.revolutionanalytics.com/2017/10/snow-leopards.html). How a conservation trust used motion-detection traps and convolutional neural networks to track an endangered animal. \n",
    "\n",
    "## Getting help\n",
    "\n",
    "If you get stuck or just have other questions, you can contact me here:\n",
    "\n",
    "David Smith `davidsmi@microsoft.com`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load some packages required by the code below. These packages are come pre-installed in Azure Notebook sessions,\n",
    "## but if you try this code elsewhere you may need to install them first with install.packages\n",
    "library(tools)\n",
    "library(httr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "\"cannot open file 'keysds.txt': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.table(\"keysds.txt\", header = TRUE, stringsAsFactors = FALSE)",
      "2. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "## Retrieve API keys and region from keys.txt file. You'll need to upload a keys.txt file with your own keys. This process\n",
    "## is explained in the README.md file in this folder.\n",
    "\n",
    "keys <- read.table(\"keys.txt\", header=TRUE, stringsAsFactors = FALSE)\n",
    "vision_api_key <- keys[\"vision\",1]\n",
    "azure_region <- keys[\"region\",1]\n",
    "vision_api_endpoint <- paste0(\"https://\", azure_region, \".api.cognitive.microsoft.com/vision/v1.0\")\n",
    "cat(\"The region is:\",azure_region,\"\\n\")\n",
    "\n",
    "## If you see ERROR-EDIT-KEYS.txt-FILE here, you need to edit keys.txt as described in README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are some URLs of example images you can try out later.\n",
    "## Feel free to find other images you want to use.\n",
    "## I visited https://en.wikipedia.org/wiki/Special:Random to go to a random Wikipedia page\n",
    "## and downloaded images from there. The Large size works with API limits\n",
    "\n",
    "example_images =c(\n",
    " ## animals\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pair_of_Merops_apiaster_feeding.jpg/1200px-Pair_of_Merops_apiaster_feeding.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/4/4f/Queenie.JPG', \n",
    " 'https://upload.wikimedia.org/wikipedia/commons/3/34/Ectopsocus_briggsi.jpg',\n",
    " ## buildings, workplaces\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Prze%C5%82%C4%99cz_Okraj-przejscie_graniczne.jpg/1200px-Prze%C5%82%C4%99cz_Okraj-przejscie_graniczne.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Wasseiges_JPG04.jpg/1200px-Wasseiges_JPG04.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/5/58/St_george_edgbaston.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/0/02/Atlanta_College_of_Art_Print_Making_Studio.jpg',\n",
    " ## non-photos \n",
    " 'https://upload.wikimedia.org/wikipedia/commons/1/15/M15_%28Ukraine%29.png',\n",
    " ## people, faces\n",
    " 'https://upload.wikimedia.org/wikipedia/en/1/1b/I_Remember_You_%28John_Hicks_album%29.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg/1200px-FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg',\n",
    " 'https://upload.wikimedia.org/wikipedia/en/d/d7/Grover_Washabaugh.jpg',\n",
    " ## things that make the API throw errors\n",
    " 'https://upload.wikimedia.org/wikipedia/commons/3/3a/FIS_Ski_Jumping_World_Cup_2014_-_Engelberg_-_20141221_-_Shohei_Tochimoto.jpg', # too large\n",
    " 'error' #malformed URL\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/commons/4/4f/Queenie.JPG \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'a man riding a horse through a fence'"
      ],
      "text/latex": [
       "'a man riding a horse through a fence'"
      ],
      "text/markdown": [
       "'a man riding a horse through a fence'"
      ],
      "text/plain": [
       "[1] \"a man riding a horse through a fence\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.895979678208752"
      ],
      "text/latex": [
       "0.895979678208752"
      ],
      "text/markdown": [
       "0.895979678208752"
      ],
      "text/plain": [
       "[1] 0.8959797"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## In this section, we'll call the Computer Vision API manually\n",
    "## Later, we'll write a function to automate the process\n",
    "\n",
    "#image_url =\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ef4d0bc7-2c45-4d17-afb1-9cad8f293657.jpg\"\n",
    "image_url = example_images[2] \n",
    "## feel free to try a different example, or provide a URL of your own choice\n",
    "\n",
    "visualFeatures = \"Description,Tags,Categories,Faces\"\n",
    "# choose the image information to return\n",
    "# options = \"Categories, Tags, Description, Faces, ImageType, Color, Adult\"\n",
    "\n",
    "details = \"Landmarks,Celebrities\"\n",
    "# Ask the Computer Vision API to detect names of celebrities or famous landmarks\n",
    "\n",
    "reqURL = paste0(vision_api_endpoint,\n",
    "               \"/analyze?visualFeatures=\",\n",
    "               visualFeatures,\n",
    "               \"&details=\",\n",
    "               details)\n",
    "\n",
    "APIresponse = POST(url = reqURL,\n",
    "                   content_type('application/json'),\n",
    "                   add_headers(.headers = c('Ocp-Apim-Subscription-Key' = vision_api_key)),\n",
    "                   body=list(url = image_url),\n",
    "                   encode = \"json\") \n",
    "\n",
    "df = content(APIresponse)\n",
    "\n",
    "## display caption and confidence\n",
    "cat(image_url,\"\\n\")\n",
    "df$description$captions[[1]]$text\n",
    "df$description$captions[[1]]$confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore that `df` object to see what other information is returned by the API (try: `print(df)`). We'll just be looking at the\n",
    "generated image caption for now.\n",
    "\n",
    "Let's define a function in R to apply the Computer Vision API to an image in a URL, and print out the image caption returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption <- function(URL) {\n",
    " reqURL = paste0(vision_api_endpoint,\n",
    "                 \"/analyze?visualFeatures=Description\",\n",
    "                 \"&details=Celebrities,Landmarks\")\n",
    " \n",
    " APIresponse = POST(url = reqURL,\n",
    "                    content_type('application/json'),\n",
    "                    add_headers(.headers = c('Ocp-Apim-Subscription-Key' = vision_api_key)),\n",
    "                    body=list(url = URL),\n",
    "                    encode = \"json\") \n",
    " \n",
    " df = content(APIresponse)\n",
    " cat(URL, \"\\n\")\n",
    "\n",
    "  ## when we get Wikimedia Commons images later, we'll grab their description too, and display it if so\n",
    " if(!is.null(attr(URL,\"desc\"))) \n",
    "  cat(\"Wikimedia Commons description:\\n\", attr(URL,\"desc\"),  \"\\n\")\n",
    "\n",
    " cat(\"Vision API description:\\n\",  df$description$captions[[1]]$text,\"\\n\")\n",
    " cat(paste0(\"Confidence: \",df$description$captions[[1]]$confidence,\"\\n\"))\n",
    " invisible(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://media.timeout.com/images/100004257/630/472/image.jpg \n",
      "Vision API description:\n",
      " a large white building with Sacré-Cœur, Paris in the background \n",
      "Confidence: 0.851741790329111\n"
     ]
    }
   ],
   "source": [
    "image_caption(\"http://media.timeout.com/images/100004257/630/472/image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some more images. We can write a function to return the URL of a random image in Wikimedia Commons, which will\n",
    "give us unlimited images to work with. We'll also check that the image meets the Computer Vision API restrictions \n",
    "(minimum dimensions 50x50, maximum file size 4Mb, certain image formats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image <- function() {\n",
    " ## Return the URL of a random image in Wikimedia Commons\n",
    " random_query <- paste0(\"https://commons.wikimedia.org/w/api.php?\",\n",
    "                        \"action=query\",\n",
    "                        \"&generator=random\", # get a random page\n",
    "                        \"&grnlimit=1\",       # return 1 page\n",
    "                        \"&grnnamespace=6\",   # category: File\n",
    "                        \"&prop=imageinfo\",\n",
    "                        \"&iiprop=url|size|extmetadata\",\n",
    "                        \"&iiurlheight=1080\",  # limit images height (sometimes)\n",
    "                        \"&format=json&formatversion=2\")\n",
    " random_response <- POST(random_query)\n",
    " output <- content(random_response)\n",
    " url <- output$query$pages[[1]]$imageinfo[[1]]$url\n",
    "\n",
    " ## check the image metadata, and throw an error if it won't work with the \n",
    " ## Computer Vision API\n",
    " ext <- tolower(file_ext(url))\n",
    " w <- output$query$pages[[1]]$imageinfo[[1]]$width\n",
    " h <- output$query$pages[[1]]$imageinfo[[1]]$height\n",
    " size <- output$query$pages[[1]]$imageinfo[[1]]$size\n",
    " desc <- output$query$pages[[1]]$imageinfo[[1]]$extmetadata$ImageDescription$value \n",
    " if(w<50 || h<50) stop(\"Image too small\") \n",
    " if(size > 4000000) stop(\"Image too large\")\n",
    " if(!(ext %in% c(\"jpg\",\"jpeg\",\"png\",\"gif\",\"bmp\"))) stop(paste(\"invalid image type:\",ext))\n",
    "\n",
    " ## In addition to the URL, return the dimensions and Wikimedia description as attributes\n",
    " attr(url, \"dims\") <- c(w=w,h=h)\n",
    " attr(url, \"desc\") <- desc\n",
    " url\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/commons/6/65/Hirota_Satsuma_equations_traveling_wave_plot_12.gif \n",
      "Wikimedia Commons description:\n",
      " Hirota Satsuma equations traveling wave plot \n",
      "Vision API description:\n",
      " a close up of a mans face on a table \n",
      "Confidence: 0.0588355807806432\n"
     ]
    }
   ],
   "source": [
    "u <- random_image()\n",
    "image_caption(u)\n",
    "\n",
    "# You might see an \"image too large\" or other error; if that happens just run this chunk again to try a different image\n",
    "# In some instances you may get no output from the Vision API. This is likely caused by an image of the wrong format or size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
